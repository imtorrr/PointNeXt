# data augmentation
dataset:
  common:
    NAME: NIBIO_MLS
    data_root: data/NIBIO_MLS
    voxel_size: 0.04
    as_pyg: True

  train:
    split: train
    loop: 30  # here, the training has been looped 30 times. therefore, the training epochs do not need much.
    presample: True
  val:
    split: val
    presample: True 
  test:
    split: test
    presample: False 

feature_keys: pos
num_classes: 4
batch_size: 4
val_batch_size: 1

dataloader:
  num_workers: 6

datatransforms:
  train: [PointsToTensor, FixedPoints, RandomDropout, PointCloudScaling, PointCloudXYZAlign, PointCloudRotation, PointCloudJitter,]
  val: [PointsToTensor, FixedPoints, PointCloudXYZAlign]
  kwargs:
    num_points: 20000
    gravity_dim: 2
    scale: [0.9, 1.1]
    angle: [0.25, 0.25, 1]
    jitter_sigma: 0.005
    jitter_clip: 0.02

# ---------------------------------------------------------------------------- #
# Training cfgs
# ---------------------------------------------------------------------------- #
val_fn: validate
ignore_index: null 
epochs: 100

cls_weighed_loss: False

criterion_args:
  NAME: CrossEntropy
  label_smoothing: 0.2

optimizer:
 NAME: 'adamw'  # performs 1 point better than adam
 weight_decay: 1.0e-4

# lr_scheduler:
sched: cosine
warmup_epochs: 0

min_lr: 1.0e-5 #
lr: 0.01 # LR linear rule.

grad_norm_clip: 10
use_voting: False
# ---------------------------------------------------------------------------- #
# io and misc
# ---------------------------------------------------------------------------- #
log_dir: 'nibio_mls'
save_freq: -1 # save epoch every xxx epochs, -1 only save last and best. 
val_freq: 1

wandb:
  project: PointNeXt-NIBIO
